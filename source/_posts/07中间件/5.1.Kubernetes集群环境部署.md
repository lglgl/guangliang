Kubernetes集群环境部署

### 1. 前置工作

- 分配主机地址及主机名

```text
192.168.10.91 kubernetes-master
192.168.10.92 kubernetes-node1
192.168.10.93 kubernetes-node2
```

- 设置主机名

```text
hostnamectl set-hostname master-node
```





### 2. 安装

- 官网参考文档：[安装 kubeadm | Kubernetes](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

```
kubeadm init  --apiserver-advertise-address=192.168.10.92  --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=192.0.0.0/8  --control-plane-endpoint=node92



kubeadm join node81:6443 --token u9e4ky.ebx4oeg30u7mzouc --discovery-token-ca-cert-hash sha256:6978f81d860b5378c02335faf9f6f30ef2ff50fd29b3a7d5b5440be3dde3e4cf


#打标签
kubectl label node node83 node-role.kubernetes.io/worker=worker




kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address 192.168.8.139 

```



[root@k8s-master ~]# kubeadm init --image-repository=registry.aliyuncs.com/google_containers
[init] Using Kubernetes version: v1.30.3
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR CRI]: container runtime is not running: output: time="2024-07-18T11:42:04+08:00" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master ~]# docker status
docker: 'status' is not a docker command.
See 'docker --help'
[root@k8s-master ~]# docker list
docker: 'list' is not a docker command.
See 'docker --help'
[root@k8s-master ~]# docker -a
unknown shorthand flag: 'a' in -a
See 'docker --help'.

Usage:  docker [OPTIONS] COMMAND

A self-sufficient runtime for containers

Common Commands:
  run         Create and run a new container from an image
  exec        Execute a command in a running container
  ps          List containers
  build       Build an image from a Dockerfile
  pull        Download an image from a registry
  push        Upload an image to a registry
  images      List images
  login       Log in to a registry
  logout      Log out from a registry
  search      Search Docker Hub for images
  version     Show the Docker version information
  info        Display system-wide information

Management Commands:
  builder     Manage builds
  buildx*     Docker Buildx
  compose*    Docker Compose
  container   Manage containers
  context     Manage contexts
  image       Manage images
  manifest    Manage Docker image manifests and manifest lists
  network     Manage networks
  plugin      Manage plugins
  system      Manage Docker
  trust       Manage trust on Docker images
  volume      Manage volumes

Swarm Commands:
  swarm       Manage Swarm

Commands:
  attach      Attach local standard input, output, and error streams to a running container
  commit      Create a new image from a container's changes
  cp          Copy files/folders between a container and the local filesystem
  create      Create a new container
  diff        Inspect changes to files or directories on a container's filesystem
  events      Get real time events from the server
  export      Export a container's filesystem as a tar archive
  history     Show the history of an image
  import      Import the contents from a tarball to create a filesystem image
  inspect     Return low-level information on Docker objects
  kill        Kill one or more running containers
  load        Load an image from a tar archive or STDIN
  logs        Fetch the logs of a container
  pause       Pause all processes within one or more containers
  port        List port mappings or a specific mapping for the container
  rename      Rename a container
  restart     Restart one or more containers
  rm          Remove one or more containers
  rmi         Remove one or more images
  save        Save one or more images to a tar archive (streamed to STDOUT by default)
  start       Start one or more stopped containers
  stats       Display a live stream of container(s) resource usage statistics
  stop        Stop one or more running containers
  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE
  top         Display the running processes of a container
  unpause     Unpause all processes within one or more containers
  update      Update configuration of one or more containers
  wait        Block until one or more containers stop, then print their exit codes

Global Options:
      --config string      Location of client config files (default "/root/.docker")
  -c, --context string     Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with "docker context use")
  -D, --debug              Enable debug mode
  -H, --host list          Daemon socket to connect to
  -l, --log-level string   Set the logging level ("debug", "info", "warn", "error", "fatal") (default "info")
      --tls                Use TLS; implied by --tlsverify
      --tlscacert string   Trust certs signed only by this CA (default "/root/.docker/ca.pem")
      --tlscert string     Path to TLS certificate file (default "/root/.docker/cert.pem")
      --tlskey string      Path to TLS key file (default "/root/.docker/key.pem")
      --tlsverify          Use TLS and verify the remote
  -v, --version            Print version information and quit

Run 'docker COMMAND --help' for more information on a command.

For more help on how to use Docker, head to https://docs.docker.com/go/guides/

[root@k8s-master ~]# systemctl enable docker
[root@k8s-master ~]# systemctl start docker
[root@k8s-master ~]# systemctl start kubelet
[root@k8s-master ~]# kubeadm init --image-repository=registry.aliyuncs.com/google_containers
[init] Using Kubernetes version: v1.30.3
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR CRI]: container runtime is not running: output: time="2024-07-18T11:43:43+08:00" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master ~]# ^[[200~systemctl status docker~
bash: systemctl: 未找到命令...
[root@k8s-master ~]# systemctl status docker
● docker.service - Docker Application Container Engine
     Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; preset: disabled)
     Active: active (running) since Thu 2024-07-18 11:40:56 +08; 6min ago
TriggeredBy: ● docker.socket
       Docs: https://docs.docker.com
   Main PID: 1362 (dockerd)
      Tasks: 14
     Memory: 107.9M
        CPU: 1.221s
     CGroup: /system.slice/docker.service
             └─1362 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

7月 18 11:40:54 k8s-master systemd[1]: Starting Docker Application Container Engine...
7月 18 11:40:55 k8s-master dockerd[1362]: time="2024-07-18T11:40:55.400360238+08:00" level=info msg="Starting up"
7月 18 11:40:55 k8s-master dockerd[1362]: time="2024-07-18T11:40:55.569636172+08:00" level=info msg="[graphdriver] using prior storage driver: overlay2"
7月 18 11:40:55 k8s-master dockerd[1362]: time="2024-07-18T11:40:55.570992718+08:00" level=info msg="Loading containers: start."
7月 18 11:40:56 k8s-master dockerd[1362]: time="2024-07-18T11:40:56.289285294+08:00" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used t>
7月 18 11:40:56 k8s-master dockerd[1362]: time="2024-07-18T11:40:56.367326521+08:00" level=info msg="Loading containers: done."
7月 18 11:40:56 k8s-master dockerd[1362]: time="2024-07-18T11:40:56.441440097+08:00" level=info msg="Docker daemon" commit=662f78c containerd-snapshotter=false storage-driver=overlay2 version=27.0.3
7月 18 11:40:56 k8s-master dockerd[1362]: time="2024-07-18T11:40:56.441737838+08:00" level=info msg="Daemon has completed initialization"
7月 18 11:40:56 k8s-master systemd[1]: Started Docker Application Container Engine.
7月 18 11:40:56 k8s-master dockerd[1362]: time="2024-07-18T11:40:56.511546955+08:00" level=info msg="API listen on /run/docker.sock"

[root@k8s-master ~]# cat <<EOF > daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "registry-mirrors": ["https://ud6340vz.mirror.aliyuncs.com"]
}
EOF
[root@k8s-master ~]# mv daemon.json /etc/docker/
[root@k8s-master ~]# systemctl daemon-reload
[root@k8s-master ~]# systemctl restart docker
[root@k8s-master ~]# kubeadm reset
W0718 11:48:59.154058    7214 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0718 11:49:01.232343    7214 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Deleted contents of the etcd data directory: /var/lib/etcd
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
W0718 11:49:01.261095    7214 cleanupnode.go:106] [reset] Failed to remove containers: output: time="2024-07-18T11:49:01+08:00" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
[root@k8s-master ~]# kubeadm init --image-repository=registry.aliyuncs.com/google_containers
[init] Using Kubernetes version: v1.30.3
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR CRI]: container runtime is not running: output: time="2024-07-18T11:49:17+08:00" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master ~]# kubeadm reset
W0718 11:51:23.822666    7273 preflight.go:56] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0718 11:51:25.419301    7273 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Deleted contents of the etcd data directory: /var/lib/etcd
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
W0718 11:51:25.446124    7273 cleanupnode.go:106] [reset] Failed to remove containers: output: time="2024-07-18T11:51:25+08:00" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
[root@k8s-master ~]# kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address 192.168.8.139
[init] Using Kubernetes version: v1.30.3
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR CRI]: container runtime is not running: output: time="2024-07-18T11:54:18+08:00" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master ~]# systemctl stop docker
Warning: Stopping docker.service, but it can still be activated by:
  docker.socket
[root@k8s-master ~]# systemctl disable docker
Removed "/etc/systemd/system/multi-user.target.wants/docker.service".
[root@k8s-master ~]# reboot
[root@k8s-master ~]#
Remote side unexpectedly closed network connection

──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Session stopped
    - Press <Return> to exit tab
    - Press R to restart session
    - Press S to save terminal output to file
    ┌──────────────────────────────────────────────────────────────────────┐
    │                 • MobaXterm Personal Edition v24.2 •                 │
    │               (SSH client, X server and network tools)               │
    │                                                                      │
    │ ⮞ SSH session to root@192.168.8.139                                  │
    │   • Direct SSH      :  ✓                                             │
    │   • SSH compression :  ✓                                             │
    │   • SSH-browser     :  ✓                                             │
    │   • X11-forwarding  :  ✓  (remote display is forwarded through SSH)  │
    │                                                                      │
    │ ⮞ For more info, ctrl+click on help or visit our website.            │
    └──────────────────────────────────────────────────────────────────────┘

Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Jul 18 11:41:29 2024 from 192.168.8.1
[root@k8s-master ~]# cat /etc/yum.repos.d/docker-ce.repo
[docker-ce-stable]
name=Docker CE Stable - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/stable
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-stable-debuginfo]
name=Docker CE Stable - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/stable
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-stable-source]
name=Docker CE Stable - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/stable
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test]
name=Docker CE Test - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/test
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test-debuginfo]
name=Docker CE Test - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/test
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-test-source]
name=Docker CE Test - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/test
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly]
name=Docker CE Nightly - $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/$basearch/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly-debuginfo]
name=Docker CE Nightly - Debuginfo $basearch
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/debug-$basearch/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-nightly-source]
name=Docker CE Nightly - Sources
baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$releasever/source/nightly
enabled=0
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg
[root@k8s-master ~]# yum install -y docker
上次元数据过期检查：0:01:39 前，执行于 2024年07月18日 星期四 11时55分01秒。
错误：
 问题: 安装的软件包的问题 docker-ce-cli-1:27.0.3-1.el9.x86_64
  - package docker-ce-cli-1:27.0.3-1.el9.x86_64 from @System conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.15-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.16-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.17-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.18-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.19-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.20-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.21-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.22-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.23-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:20.10.24-3.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:23.0.0-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:23.0.1-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:23.0.2-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:23.0.4-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:23.0.5-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:23.0.6-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.0-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.1-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.2-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.3-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.4-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.5-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.6-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.7-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.8-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:24.0.9-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:25.0.0-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:25.0.1-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:25.0.2-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:25.0.3-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:25.0.4-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:25.0.5-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:26.0.0-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:26.0.1-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:26.0.2-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:26.1.0-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:26.1.1-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:26.1.2-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:26.1.3-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:26.1.4-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:27.0.1-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:27.0.2-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - package docker-ce-cli-1:27.0.3-1.el9.x86_64 from docker-ce-stable conflicts with docker provided by podman-docker-2:5.1.1-1.el9.noarch from appstream
  - 无法为该任务安装最佳候选
(尝试在命令行中添加 '--allowerasing' 来替换冲突的软件包 或 '--skip-broken' 来跳过无法安装的软件包 或 '--nobest' 来不只使用软件包的最佳候选)
[root@k8s-master ~]# systemctl start docker && systemctl enable docker
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.
[root@k8s-master ~]# yum install -y containerd
上次元数据过期检查：0:02:38 前，执行于 2024年07月18日 星期四 11时55分01秒。
软件包 containerd.io-1.7.18-3.1.el9.x86_64 已安装。
依赖关系解决。
==============================================================================================================================================================================================================
 软件包                                            架构                                       版本                                                 仓库                                                  大小
==============================================================================================================================================================================================================
升级:
 containerd.io                                     x86_64                                     1.7.19-3.1.el9                                       docker-ce-stable                                      43 M

事务概要
==============================================================================================================================================================================================================
升级  1 软件包

总下载：43 M
下载软件包：
containerd.io-1.7.19-3.1.el9.x86_64.rpm                                                                                                                                       1.4 MB/s |  43 MB     00:30
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
总计                                                                                                                                                                          1.4 MB/s |  43 MB     00:30
运行事务检查
事务检查成功。
运行事务测试
事务测试成功。
运行事务
  准备中  :                                                                                                                                                                                               1/1
  升级    : containerd.io-1.7.19-3.1.el9.x86_64                                                                                                                                                           1/2
  运行脚本: containerd.io-1.7.19-3.1.el9.x86_64                                                                                                                                                           1/2
  运行脚本: containerd.io-1.7.18-3.1.el9.x86_64                                                                                                                                                           2/2
  清理    : containerd.io-1.7.18-3.1.el9.x86_64                                                                                                                                                           2/2
  运行脚本: containerd.io-1.7.18-3.1.el9.x86_64                                                                                                                                                           2/2
  验证    : containerd.io-1.7.19-3.1.el9.x86_64                                                                                                                                                           1/2
  验证    : containerd.io-1.7.18-3.1.el9.x86_64                                                                                                                                                           2/2

已升级:
  containerd.io-1.7.19-3.1.el9.x86_64

完毕！
[root@k8s-master ~]# mkdir -p /etc/containerd
[root@k8s-master ~]# containerd config default | sudo tee /etc/containerd/config.toml
disabled_plugins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd"
state = "/run/containerd"
temp = ""
version = 2

[cgroup]
  path = ""

[debug]
  address = ""
  format = ""
  gid = 0
  level = ""
  uid = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
  tcp_address = ""
  tcp_tls_ca = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0

[metrics]
  address = ""
  grpc_histogram = false

[plugins]

  [plugins."io.containerd.gc.v1.scheduler"]
    deletion_threshold = 0
    mutation_threshold = 100
    pause_threshold = 0.02
    schedule_delay = "0s"
    startup_delay = "100ms"

  [plugins."io.containerd.grpc.v1.cri"]
    cdi_spec_dirs = ["/etc/cdi", "/var/run/cdi"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    drain_exec_sync_io_timeout = "0s"
    enable_cdi = false
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_deprecation_warnings = []
    ignore_image_defined_volumes = false
    image_pull_progress_timeout = "5m0s"
    image_pull_with_sync_fs = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.k8s.io/pause:3.8"
    selinux_category_range = 1024
    stats_collect_period = 10
    stream_idle_timeout = "4h0m0s"
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    systemd_cgroup = false
    tolerate_missing_hugetlb_controller = true
    unset_seccomp_profile = ""

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      conf_template = ""
      ip_pref = ""
      max_conf_num = 1
      setup_serially = false

    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      ignore_blockio_not_enabled_errors = false
      ignore_rdt_not_enabled_errors = false
      no_pivot = false
      snapshotter = "overlayfs"

      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          privileged_without_host_devices_all_devices_allowed = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"
          sandbox_mode = "podsandbox"
          snapshotter = ""

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = false

      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = "node"

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = ""

      [plugins."io.containerd.grpc.v1.cri".registry.auths]

      [plugins."io.containerd.grpc.v1.cri".registry.configs]

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]

    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""

  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"

  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"

  [plugins."io.containerd.internal.v1.tracing"]

  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"

  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false

  [plugins."io.containerd.nri.v1.nri"]
    disable = true
    disable_connections = false
    plugin_config_path = "/etc/nri/conf.d"
    plugin_path = "/opt/nri/plugins"
    plugin_registration_timeout = "5s"
    plugin_request_timeout = "2s"
    socket_path = "/var/run/nri/nri.sock"

  [plugins."io.containerd.runtime.v1.linux"]
    no_shim = false
    runtime = "runc"
    runtime_root = ""
    shim = "containerd-shim"
    shim_debug = false

  [plugins."io.containerd.runtime.v2.task"]
    platforms = ["linux/amd64"]
    sched_core = false

  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]

  [plugins."io.containerd.service.v1.tasks-service"]
    blockio_config_file = ""
    rdt_config_file = ""

  [plugins."io.containerd.snapshotter.v1.aufs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.blockfile"]
    fs_type = ""
    mount_options = []
    root_path = ""
    scratch_file = ""

  [plugins."io.containerd.snapshotter.v1.devmapper"]
    async_remove = false
    base_image_size = ""
    discard_blocks = false
    fs_options = ""
    fs_type = ""
    pool_name = ""
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.native"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    mount_options = []
    root_path = ""
    sync_remove = false
    upperdir_label = false

  [plugins."io.containerd.snapshotter.v1.zfs"]
    root_path = ""

  [plugins."io.containerd.tracing.processor.v1.otlp"]

  [plugins."io.containerd.transfer.v1.local"]
    config_path = ""
    max_concurrent_downloads = 3
    max_concurrent_uploaded_layers = 3

    [[plugins."io.containerd.transfer.v1.local".unpack_config]]
      differ = ""
      platform = "linux/amd64"
      snapshotter = "overlayfs"

[proxy_plugins]

[stream_processors]

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar"

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+gzip+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
  "io.containerd.timeout.bolt.open" = "0s"
  "io.containerd.timeout.metrics.shimstats" = "2s"
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[ttrpc]
  address = ""
  gid = 0
  uid = 0
[root@k8s-master ~]# systemctl restart containerd
[root@k8s-master ~]# kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address 192.168.8.139
[init] Using Kubernetes version: v1.30.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0718 12:00:05.539329    2881 checks.go:844] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.8.139]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.8.139 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.8.139 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 502.359339ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 7.001275687s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: xepz5z.gf5y8jwufu1x73gc
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.8.139:6443 --token xepz5z.gf5y8jwufu1x73gc \
        --discovery-token-ca-cert-hash sha256:0fdd941875a54fb3204e7a2257cc158e59fc279483db6d2e358459b9db85ccc1
