
firewall-cmd --list-all
docker-compose down
docker-compose down && docker-compose up

systemctl restart firewalld.service
systemctl stop firewalld.service
systemctl start firewalld.service

### es
cd /data/docker-compose/middleware/elasticsearch
docker-compose up
docker-compose down
docker-compose start
docker-compose exec prod_middleware_es_04 bash

粘贴：elastic#SZT5764
elastic#SZT5764
http://192.168.240.212:19200/_cat/nodes?pretty

### zookeeper
cd /data/docker-compose/middleware/zookeeper

docker-compose up
docker-compose exec prod_middleware_zookeeper_01 bash

prod_middleware_zookeeper_01_1  | /docker-entrypoint.sh: line 43: /conf/zoo.cfg: Permission denied


### minio

cd /data/docker-compose/middleware/minio



### nacos
cd /data/docker-compose/middleware/nacos


### kafka
cd /data/docker-compose/middleware/kafka



```shell
#测试

docker exec -it prod_middleware_kafka_02 /bin/bash
cd /opt/kafka_2.13-2.8.1/bin


#生产消息，总数200W，吞吐量200000，每条消息1000字节
./kafka-producer-perf-test.sh --topic test_of_kongkong --num-records 2000000 --record-size 1000  --throughput 200000 --producer-props bootstrap.servers=192.168.240.211:19092

#消费消息，16线程，200W消息，1MB一次请求
./kafka-consumer-perf-test.sh --broker-list 192.168.240.211:19092 --topic test_of_kongkong --fetch-size 1048576 --messages 2000000 --threads 16 --reporting-interval 2000 --timeout 10000

```

## 测试记录

```shell
bash-5.1# ./kafka-producer-perf-test.sh --topic test_of_kongkong --num-records 20000000 --record-size 1000  --throughput 20000 --producer-props bootstrap.servers=192.168.240.211:19092

99989 records sent, 19993.8 records/sec (19.07 MB/sec), 0.5 ms avg latency, 1.0 ms max latency.
100020 records sent, 20000.0 records/sec (19.07 MB/sec), 0.5 ms avg latency, 13.0 ms max latency.
100020 records sent, 20004.0 records/sec (19.08 MB/sec), 0.5 ms avg latency, 1.0 ms max latency.
100020 records sent, 20004.0 records/sec (19.08 MB/sec), 0.5 ms avg latency, 14.0 ms max latency.
100020 records sent, 20004.0 records/sec (19.08 MB/sec), 8.0 ms avg latency, 239.0 ms max latency.
100012 records sent, 20002.4 records/sec (19.08 MB/sec), 0.5 ms avg latency, 14.0 ms max latency.
100008 records sent, 19997.6 records/sec (19.07 MB/sec), 0.5 ms avg latency, 10.0 ms max latency.
# 生产
bash-5.1# ./kafka-producer-perf-test.sh --topic test_of_kongkong --num-records 2000000 --record-size 1000  --throughput 200000 --producer-props bootstrap.servers=192.168.240.211:19092
log4j:ERROR Could not read configuration file from URL [file:/opt/kafka_2.13-2.8.1/bin/../config/tools-log4j.properties].
log4j:WARN No appenders could be found for logger (org.apache.kafka.clients.producer.ProducerConfig).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
447585 records sent, 89517.0 records/sec (85.37 MB/sec), 304.9 ms avg latency, 571.0 ms max latency.
563120 records sent, 112624.0 records/sec (107.41 MB/sec), 292.3 ms avg latency, 315.0 ms max latency.
566016 records sent, 113203.2 records/sec (107.96 MB/sec), 289.3 ms avg latency, 313.0 ms max latency.
2000000 records sent, 104942.806171 records/sec (100.08 MB/sec), 298.59 ms avg latency, 579.00 ms max latency, 290 ms 50th, 323 ms 95th, 565 ms 99th, 576 ms 99.9th.

# 消费
bash-5.1# ./kafka-consumer-perf-test.sh --broker-list 192.168.240.211:19092 --topic test_of_kongkong --fetch-size 1048576 --messages 2000000 --threads 16 --reporting-interval 2000 --timeout 10000
WARNING: option [threads] and [num-fetch-threads] have been deprecated and will be ignored by the test
log4j:ERROR Could not read configuration file from URL [file:/opt/kafka_2.13-2.8.1/bin/../config/tools-log4j.properties].
java.io.FileNotFoundException: /opt/kafka_2.13-2.8.1/bin/../config/tools-log4j.properties (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
        at java.io.FileInputStream.<init>(FileInputStream.java:93)
        at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
        at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
        at org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)
        at kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala:25)
        at kafka.utils.CommandLineUtils$.<clinit>(CommandLineUtils.scala:28)
        at kafka.tools.ConsumerPerformance$ConsumerPerfConfig.<init>(ConsumerPerformance.scala:269)
        at kafka.tools.ConsumerPerformance$.main(ConsumerPerformance.scala:44)
        at kafka.tools.ConsumerPerformance.main(ConsumerPerformance.scala)
log4j:ERROR Ignoring configuration file [file:/opt/kafka_2.13-2.8.1/bin/../config/tools-log4j.properties].
log4j:WARN No appenders could be found for logger (kafka.utils.Log4jControllerRegistration$).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec
2023-07-25 15:21:05:705, 2023-07-25 15:21:29:111, 1907.5508, 81.4984, 2000212, 85457.2332, 3692, 19714, 96.7612, 101461.4994




```


prod_cluster_kafka_01_1  | [Configuring] 'default.replication.factor' in '/opt/kafka/config/server.properties'
prod_cluster_kafka_01_1  | [Configuring] 'advertised.listeners' in '/opt/kafka/config/server.properties'
prod_cluster_kafka_01_1  | [Configuring] 'port' in '/opt/kafka/config/server.properties'
prod_cluster_kafka_01_1  | Excluding KAFKA_HOME from broker config
prod_cluster_kafka_01_1  | [Configuring] 'log.dirs' in '/opt/kafka/config/server.properties'
prod_cluster_kafka_01_1  | [Configuring] 'listeners' in '/opt/kafka/config/server.properties'
prod_cluster_kafka_01_1  | Excluding KAFKA_VERSION from broker config
prod_cluster_kafka_01_1  | [Configuring] 'zookeeper.connect' in '/opt/kafka/config/server.properties'
prod_cluster_kafka_01_1  | [Configuring] 'broker.id' in '/opt/kafka/config/server.properties'



### redis
cd /data/docker-compose/middleware/redis

prod_middleware_redis_01_1  | 1:C 25 Jul 2023 07:24:39.875 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
prod_middleware_redis_01_1  | 1:C 25 Jul 2023 07:24:39.875 # Redis version=7.0.11, bits=64, commit=00000000, modified=0, pid=1, just started
prod_middleware_redis_01_1  | 1:C 25 Jul 2023 07:24:39.875 # Configuration loaded
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.875 * monotonic clock: POSIX clock_gettime
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.876 * No cluster configuration found, I'm 69023301118e3f09059f74c38ad261294b0a6d93
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.877 * Running mode=cluster, port=16379.
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.877 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.877 # Server initialized
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.877 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.879 * Creating AOF base file appendonly.aof.1.base.rdb on server start
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.879 * Creating AOF incr file appendonly.aof.1.incr.aof on server start
prod_middleware_redis_01_1  | 1:M 25 Jul 2023 07:24:39.879 * Ready to accept connections


echo "net.core.somaxconn = 1024" >> /etc/sysctl.conf
echo "vm.overcommit_memory = 1" >> /etc/sysctl.conf
cat /etc/sysctl.conf
sysctl -p


docker exec -it redis_prod_middleware_redis_01_1 /bin/bash


redis-cli -h 192.168.240.211 -p 16379 -a redis#SZT7658 cluster nodes


redis-cli --cluster create 192.168.240.211:16379 192.168.240.212:16379 192.168.240.213:16379 192.168.240.214:16379 192.168.240.215:16379 192.168.240.216:16379 --cluster-replicas 1 -a redis#SZT7658
